"""èŠå¤©ç•Œé¢ç»„ä»¶."""

from textual.containers import Vertical
from textual.widgets import Input, RichLog, Static

from selfme.core.llm import LLMClient
from selfme.core.memory import MemoryStore


class ChatContainer(Vertical):
    """èŠå¤©å®¹å™¨ç»„ä»¶."""

    DEFAULT_CSS = """
    ChatContainer {
        height: 100%;
    }

    #chat-history {
        height: 1fr;
        border: solid $primary-darken-2;
        padding: 1;
        background: $surface-darken-2;
    }

    #chat-input {
        height: 3;
        margin: 0;
    }

    #chat-input:focus {
        border: tall $primary;
    }

    .user-message {
        color: $text;
        background: $primary-darken-3;
        padding: 0 1;
        margin: 0 0 1 0;
    }

    .assistant-message {
        color: $text;
        background: $surface-darken-1;
        padding: 0 1;
        margin: 0 0 1 0;
    }
    """

    def __init__(self):
        super().__init__()
        self.memory = MemoryStore()
        self.llm = None
        self.is_generating = False

    def compose(self):
        """æ„å»ºç»„ä»¶."""
        # èŠå¤©å†å²æ˜¾ç¤ºåŒº
        yield RichLog(id="chat-history", highlight=True, wrap=True)
        # è¾“å…¥æ¡†
        yield Input(placeholder="è¾“å…¥æ¶ˆæ¯ï¼ŒæŒ‰ Enter å‘é€...", id="chat-input")

    def on_mount(self):
        """ç»„ä»¶æŒ‚è½½æ—¶åˆå§‹åŒ–."""
        try:
            self.llm = LLMClient()
            self.add_system_message(
                f"ğŸ¦ æ¬¢è¿å›æ¥ï¼Œä¸ƒé“å¸ˆï¼\n"
                f"ğŸ™ SelfMe v0.1.0 å·²å°±ç»ª\n"
                f"[dim]æ¨¡å‹: {self.llm.model}[/dim]"
            )
        except ValueError as e:
            self.add_system_message(f"âš ï¸ åˆå§‹åŒ–å¤±è´¥: {e}\nè¯·æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„ OPENAI_API_KEY")

    def on_input_submitted(self, event: Input.Submitted):
        """å¤„ç†è¾“å…¥æäº¤."""
        if not event.value.strip() or self.is_generating:
            return

        user_message = event.value.strip()

        # æ¸…ç©ºè¾“å…¥æ¡†
        input_widget = self.query_one("#chat-input", Input)
        input_widget.value = ""

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        self.add_user_message(user_message)

        # è°ƒç”¨ LLM ç”Ÿæˆå›å¤
        self.generate_response(user_message)

    def add_user_message(self, content: str):
        """æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°æ˜¾ç¤ºåŒº."""
        history = self.query_one("#chat-history", RichLog)
        history.write(f"[b]ä½ :[/b] {content}")
        self.memory.add("user", content)

    def add_assistant_message(self, content: str):
        """æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°æ˜¾ç¤ºåŒº."""
        history = self.query_one("#chat-history", RichLog)
        history.write(f"[b]ğŸ™:[/b] {content}")
        self.memory.add("assistant", content)

    def add_system_message(self, content: str):
        """æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯."""
        history = self.query_one("#chat-history", RichLog)
        history.write(f"[dim]{content}[/dim]")

    def generate_response(self, user_message: str):
        """ç”Ÿæˆ LLM å›å¤ (æµå¼)."""
        if not self.llm:
            return

        self.is_generating = True
        history = self.query_one("#chat-history", RichLog)

        # è·å–å®Œæ•´ä¸Šä¸‹æ–‡
        messages = self.memory.to_llm_format(n=10)  # æœ€è¿‘10æ¡ä½œä¸ºä¸Šä¸‹æ–‡

        # æµå¼ç”Ÿæˆï¼Œæ”¶é›†å®Œæ•´å“åº”
        full_response = ""
        for token in self.llm.chat(messages, stream=True):
            full_response += token

        # ä¸€æ¬¡æ€§æ˜¾ç¤ºå®Œæ•´å›å¤
        history.write(f"[b]ğŸ™:[/b] {full_response}")
        self.memory.add("assistant", full_response)
        self.is_generating = False

    def clear_chat(self):
        """æ¸…ç©ºå¯¹è¯."""
        self.memory.clear()
        history = self.query_one("#chat-history", RichLog)
        history.clear()
        self.add_system_message("ğŸ—‘ï¸ å¯¹è¯å·²æ¸…ç©º")
